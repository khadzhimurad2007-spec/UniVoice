import { useEffect, useRef, useState } from 'react';
import { SpeechRecognizer } from '../voice/recognizer';
import { routeCommand } from '../voice/commandRouter';
import { NLU } from '../voice/nlu';

export function useVoice({ navigate, tts = true }) {
    const [isSupported, setIsSupported] = useState(!!(window.SpeechRecognition || window.webkitSpeechRecognition));
    const [isListening, setIsListening] = useState(false);
    const [lastPhrase, setLastPhrase] = useState('');
    const [status, setStatus] = useState('idle');
    const [isSpeaking, setIsSpeaking] = useState(false);

    const recogRef = useRef(null);
    const nluRef = useRef(null);
    const listeningRef = useRef(false);
    const wakeWordDetectedRef = useRef(false);

    const speak = (text) => {
        if (!tts || !window.speechSynthesis) return;

        window.speechSynthesis.cancel();

        const utter = new SpeechSynthesisUtterance(text);
        utter.lang = 'ru-RU';

        setIsSpeaking(true);

        // Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ñ€ÐµÑ‡Ð¸
        if (listeningRef.current) {
            recogRef.current?.stop();
        }

        window.speechSynthesis.speak(utter);

        utter.onend = () => {
            setIsSpeaking(false);
            // Ð’Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ»Ðµ Ñ€ÐµÑ‡Ð¸, ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð¸ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ listening
            if (listeningRef.current) {
                setTimeout(() => {
                    try {
                        recogRef.current?.start();
                    } catch (e) {
                        console.log('Restart after speech failed:', e);
                    }
                }, 500);
            }
        };
    };

    const startListening = () => {
        if (!recogRef.current || isSpeaking) return;
        try {
            recogRef.current.start();
            setIsListening(true);
            listeningRef.current = true;
            setStatus('listening');
            wakeWordDetectedRef.current = false;
        } catch (e) {
            console.error('Start listening error:', e);
        }
    };

    const stopListening = () => {
        if (!recogRef.current) return;
        try {
            recogRef.current.stop();
            setIsListening(false);
            listeningRef.current = false;
            setStatus('idle');
            wakeWordDetectedRef.current = false;
        } catch (e) {
            console.error('Stop listening error:', e);
        }
    };

    useEffect(() => {
        try {
            recogRef.current = new SpeechRecognizer({
                lang: 'ru-RU',
                continuous: true,
                interimResults: true,
                onStart: () => setStatus('listening'),
                onEnd: () => {
                    if (!isSpeaking) {
                        setStatus(listeningRef.current ? 'listening' : 'idle');
                    }
                },
                onError: (e) => {
                    console.error('Speech recognition error:', e);
                    setStatus('error');
                    // ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…
                    if (listeningRef.current) {
                        setTimeout(() => {
                            try {
                                recogRef.current?.start();
                            } catch (err) {
                                console.log('Auto-restart failed:', err);
                            }
                        }, 1000);
                    }
                },
                onResult: async ({ transcript, isFinal }) => {
                    // Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ñ€ÐµÑ‡Ð¸ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°
                    if (isSpeaking) return;

                    setLastPhrase(transcript);
                    const normalized = (transcript || '').toLowerCase().trim();

                    // Wake words detection - Ð”Ð•Ð¢Ð•ÐšÐ¢Ð˜Ðœ Ð¢Ð Ð˜Ð“Ð“Ð•Ð ÐÐ«Ð• Ð¡Ð›ÐžÐ’Ð ÐŸÐžÐ¡Ð¢ÐžÐ¯ÐÐÐž
                    const wakeWords = ['ÑŽÐ½Ð¸', 'ÑŽÐ½Ð¸Ð²Ð¾Ð¹Ñ', 'ÑƒÐ½Ð¸', 'ÑƒÐ½Ð¸Ð²Ð¾Ð¹Ñ', 'univoice', 'uni'];
                    const woke = wakeWords.some((w) => normalized.includes(w));

                    // ðŸ”¥ ÐšÐ›Ð®Ð§Ð•Ð’ÐžÐ• Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð•: ÐÐ²Ñ‚Ð¾Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð’Ð¡Ð•Ð“Ð”Ð, Ð´Ð°Ð¶Ðµ ÐºÐ¾Ð³Ð´Ð° Ð½Ðµ ÑÐ»ÑƒÑˆÐ°ÐµÐ¼
                    if (woke && !listeningRef.current && !isSpeaking && !wakeWordDetectedRef.current) {
                        wakeWordDetectedRef.current = true;
                        console.log('ðŸ”¥ Wake word detected, starting listening...');
                        startListening();
                        setTimeout(() => {
                            speak('Ð¡Ð»ÑƒÑˆÐ°ÑŽ Ð²Ð°Ñ.');
                        }, 300);
                        return;
                    }

                    // Ð•ÑÐ»Ð¸ Ñ€ÐµÐ¶Ð¸Ð¼ Ð½Ðµ Ð°ÐºÑ‚Ð¸Ð²ÐµÐ½ â€” Ð½Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
                    if (!listeningRef.current) return;

                    if (isFinal && normalized.length > 2) { // ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°
                        console.log('Processing command:', normalized);
                        await routeCommand({
                            phrase: transcript,
                            context: {
                                speak,
                                navigate,
                                startListening,
                                stopListening,
                                nlu: nluRef.current,
                                isSpeaking
                            }
                        });
                    }
                }
            });

            // ðŸ”¥ ÐšÐ›Ð®Ð§Ð•Ð’ÐžÐ• Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð•: Ð¡Ð ÐÐ—Ð£ Ð—ÐÐŸÐ£Ð¡ÐšÐÐ•Ðœ Ð ÐÐ¡ÐŸÐžÐ—ÐÐÐ’ÐÐÐ˜Ð• Ð”Ð›Ð¯ Ð”Ð•Ð¢Ð•ÐšÐ¦Ð˜Ð˜ Ð¢Ð Ð˜Ð“Ð“Ð•Ð ÐÐ«Ð¥ Ð¡Ð›ÐžÐ’
            console.log('Starting continuous recognition for wake word detection...');
            recogRef.current.start();

            // NLU Ñ‡ÐµÑ€ÐµÐ· OpenAI (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÐºÐ»ÑŽÑ‡)
            const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
            if (apiKey) {
                nluRef.current = new NLU({ apiKey });
            }
        } catch (e) {
            console.error('Voice hook initialization error:', e);
            setIsSupported(false);
            setStatus('error');
        }

        return () => {
            stopListening();
            recogRef.current = null;
            nluRef.current = null;
            window.speechSynthesis.cancel();
        };
    }, []);

    return {
        isSupported,
        isListening,
        lastPhrase,
        status,
        isSpeaking,
        startListening,
        stopListening,
        speak
    };
}